---
title: "package functions"
author: "Han Xiu"
date: "`r Sys.Date()`"
output: html_document
---



# 1. build_paths()

```{r}
#' Multi-path forward selection using AIC
#'
#' Build a small "forest" of alternative forward-selection paths by
#' allowing near-ties in AIC at each step.
#'
#' @param x Matrix or data.frame of predictors (n x p).
#' @param y Response vector of length n.
#' @param family Model family: "gaussian" (lm) or "binomial" (glm).
#' @param K Maximum number of forward steps (max model size).
#' @param eps Minimum AIC improvement required to expand from a parent.
#' @param delta AIC tolerance for keeping near-tie child models (per parent).
#' @param L Maximum number of models to keep per level (after deduplication).
#'
#' @return A list of class "path_forest" with components:
#'   - frontiers: list of data.frames, one per step, describing models at that step.
#'   - aic_by_model: data.frame summarizing all models across all steps.
#'   - meta: list with input parameters and variable names.
#'
#' @examples
#' \dontrun{
#'   set.seed(1)
#'   n <- 100; p <- 5
#'   X <- matrix(rnorm(n * p), n, p)
#'   colnames(X) <- paste0("x", 1:p)
#'   y <- X[,1] * 2 - X[,2] + rnorm(n)
#'
#'   forest <- build_paths(
#'     x = X, y = y, family = "gaussian",
#'     K = min(ncol(X), 5), eps = 1e-6, delta = 1, L = 20
#'   )
#' }
#' 
#' @export
build_paths <- function(x, y,
                        family = c("gaussian", "binomial"),
                        K = min(ncol(x), 10),
                        eps = 1e-6,
                        delta = 1,
                        L = 50) {

  ## ---- Input checks and preparation ----

  family <- match.arg(family)

  # Coerce x to data.frame and ensure column names
  if (is.matrix(x)) {
    x <- as.data.frame(x)
  } else {
    x <- as.data.frame(x)
  }

  if (is.null(colnames(x))) {
    colnames(x) <- paste0("x", seq_len(ncol(x)))
  }

  n <- NROW(x)
  p <- NCOL(x)

  if (length(y) != n) {
    stop("Length of y must match number of rows in x.")
  }

  # Build working data.frame with y + predictors
  df <- data.frame(y = y, x, check.names = FALSE)
  var_names <- colnames(x)

  # Helper: fit model and compute AIC
  fit_model_aic <- function(vars) {
    # vars: character vector of predictor names
    if (length(vars) == 0L) {
      # Intercept-only model
      form <- y ~ 1
    } else {
      form <- as.formula(
        paste("y ~", paste(vars, collapse = " + "))
      )
    }

    if (family == "gaussian") {
      fit <- stats::lm(form, data = df)
    } else {
      fit <- stats::glm(form, family = stats::binomial(), data = df)
    }
    aic_val <- stats::AIC(fit)

    list(fit = fit, aic = aic_val, formula = form)
  }

  # Helper: build a unique key for a model (sorted variable names)
  model_key <- function(vars) {
    # Always sort vars for a canonical representation
    if (length(vars) == 0L) {
      return("<empty>")
    }
    paste(sort(vars), collapse = " + ")
  }

  ## ---- Initialization: level 0 (empty model) ----

  # Fit the intercept-only model once
  empty_fit <- fit_model_aic(character(0))

  # We store current_models as a list of model descriptors
  # Each element is a list with:
  #   vars   : character vector of included predictors
  #   aic    : numeric AIC value
  #   key    : unique model key
  #   parent : key of parent model (NA for step 0)
  #   step   : step index (0 for empty)
  current_models <- list(
    list(
      vars   = character(0),
      aic    = empty_fit$aic,
      key    = model_key(character(0)),
      parent = NA_character_,
      step   = 0L,
      formula = empty_fit$formula
    )
  )

  # Frontiers: store level 0 explicitly if desired
  frontiers <- list()
  frontiers[[1]] <- data.frame(
    step   = 0L,
    model_id = seq_along(current_models),
    key    = vapply(current_models, function(m) m$key, character(1)),
    aic    = vapply(current_models, function(m) m$aic, numeric(1)),
    size   = vapply(current_models, function(m) length(m$vars), integer(1)),
    parent = vapply(current_models, function(m) m$parent, character(1)),
    vars   = I(lapply(current_models, function(m) m$vars)),
    stringsAsFactors = FALSE
  )

  # We also maintain a global table of all models encountered
  all_models <- frontiers[[1]]

  ## ---- Forward steps ----

  # Outer loop over up to K steps
  for (k in seq_len(K)) {

    # List to store all candidate children from all parents at this step
    candidate_children <- list()
    child_idx <- 1L

    # For each parent model at current frontier
    for (p_idx in seq_along(current_models)) {
      parent <- current_models[[p_idx]]
      parent_vars <- parent$vars
      parent_aic  <- parent$aic
      parent_key_ <- parent$key

      # Determine remaining variables that are not yet in parent
      remaining <- setdiff(var_names, parent_vars)
      if (length(remaining) == 0L) {
        # Cannot expand this parent further
        next
      }

      # For each remaining variable, build a candidate child model
      for (v in remaining) {
        child_vars <- sort(c(parent_vars, v))
        key <- model_key(child_vars)

        # Fit and compute AIC
        res <- fit_model_aic(child_vars)

        candidate_children[[child_idx]] <- list(
          step      = k,
          parent    = parent_key_,
          parent_aic = parent_aic,
          vars      = child_vars,
          key       = key,
          added     = v,
          aic       = res$aic,
          formula   = res$formula
        )
        child_idx <- child_idx + 1L
      }
    }

    # If no candidates at all, we stop
    if (length(candidate_children) == 0L) {
      break
    }

    # Turn candidate list into a data.frame for easier handling
    cand_df <- data.frame(
      step      = vapply(candidate_children, function(z) z$step, integer(1)),
      parent    = vapply(candidate_children, function(z) z$parent, character(1)),
      parent_aic= vapply(candidate_children, function(z) z$parent_aic, numeric(1)),
      key       = vapply(candidate_children, function(z) z$key, character(1)),
      added     = vapply(candidate_children, function(z) z$added, character(1)),
      aic       = vapply(candidate_children, function(z) z$aic,  numeric(1)),
      stringsAsFactors = FALSE
    )
    # Keep the vars/formula in parallel list (list columns are fine, but
    # here we keep them in a separate list to simplify handling).
    cand_vars    <- lapply(candidate_children, function(z) z$vars)
    cand_formula <- lapply(candidate_children, function(z) z$formula)

    ## ---- Apply delta/eps rules per parent ----

    # For each parent, we: 
    #  1) find best child AIC,
    #  2) check improvement vs parent_aic,
    #  3) keep children within delta of that best AIC if improvement >= eps.
    keep_idx <- logical(nrow(cand_df))
    parent_keys_unique <- unique(cand_df$parent)

    for (pk in parent_keys_unique) {
      idx <- which(cand_df$parent == pk)
      parent_aic_val <- cand_df$parent_aic[idx[1]]  # all same

      # Best AIC among children of this parent
      best_child_aic <- min(cand_df$aic[idx])

      # Check if best child improves AIC by at least eps
      if ((parent_aic_val - best_child_aic) >= eps) {
        # Keep all near-ties within delta
        near_idx <- idx[ cand_df$aic[idx] <= (best_child_aic + delta) ]
        keep_idx[near_idx] <- TRUE
      } else {
        # No children kept for this parent
        next
      }
    }

    # Filter candidates
    if (!any(keep_idx)) {
      # No expanding parent had sufficient improvement -> stop algorithm
      break
    }

    cand_df_kept   <- cand_df[keep_idx, , drop = FALSE]
    cand_vars_kept <- cand_vars[keep_idx]
    cand_form_kept <- cand_formula[keep_idx]

    ## ---- Deduplicate across parents: same model key ----

    # Multiple parents may produce the same child model (same var set).
    # We deduplicate by key, keeping the one with smallest AIC.
    # We'll build an index of best per key.
    unique_keys <- unique(cand_df_kept$key)
    dedup_idx   <- integer(length(unique_keys))

    for (i in seq_along(unique_keys)) {
      k_ <- unique_keys[i]
      idx <- which(cand_df_kept$key == k_)
      # Among these, pick the smallest AIC
      best_idx <- idx[ which.min(cand_df_kept$aic[idx]) ]
      dedup_idx[i] <- best_idx
    }

    dedup_idx <- sort(dedup_idx)
    dedup_df   <- cand_df_kept[dedup_idx, , drop = FALSE]
    dedup_vars <- cand_vars_kept[dedup_idx]
    dedup_form <- cand_form_kept[dedup_idx]

    ## ---- Limit to at most L models by best AIC ----

    if (nrow(dedup_df) > L) {
      order_idx <- order(dedup_df$aic)
      keep_L    <- order_idx[seq_len(L)]
      dedup_df   <- dedup_df[keep_L, , drop = FALSE]
      dedup_vars <- dedup_vars[keep_L]
      dedup_form <- dedup_form[keep_L]
    }

    # Build current_models for next iteration
    current_models <- lapply(seq_len(nrow(dedup_df)), function(i) {
      list(
        vars    = dedup_vars[[i]],
        aic     = dedup_df$aic[i],
        key     = dedup_df$key[i],
        parent  = dedup_df$parent[i],
        step    = k,
        formula = dedup_form[[i]]
      )
    })

    # Build frontier data.frame for this step
    frontier_k <- data.frame(
      step     = k,
      model_id = seq_along(current_models),
      key      = vapply(current_models, function(m) m$key, character(1)),
      aic      = vapply(current_models, function(m) m$aic, numeric(1)),
      size     = vapply(current_models, function(m) length(m$vars), integer(1)),
      parent   = vapply(current_models, function(m) m$parent, character(1)),
      vars     = I(lapply(current_models, function(m) m$vars)),
      stringsAsFactors = FALSE
    )

    frontiers[[k + 1L]] <- frontier_k
    all_models <- rbind(all_models, frontier_k)
  }

  ## ---- Assemble return object ----

  meta <- list(
    family   = family,
    K        = K,
    eps      = eps,
    delta    = delta,
    L        = L,
    n        = n,
    p        = p,
    var_names = var_names
  )

  out <- list(
    frontiers    = frontiers,
    aic_by_model = all_models,
    meta         = meta
  )
  class(out) <- "path_forest"

  out
}

```





# 2. stability()

```{r}
#' Resampling-based stability for multi-path forward selection
#'
#' For each resample of the data (bootstrap or subsample), run \code{build_paths()}
#' and compute, for each predictor, the proportion of models in that resample
#' that contain the predictor. Average these proportions across resamples to
#' obtain a stability score \eqn{\pi_j} for each predictor \eqn{j}.
#'
#' @param x Matrix or data.frame of predictors (n x p).
#' @param y Response vector of length n.
#' @param B Integer, number of resamples (e.g., 50 or 100).
#' @param resample Resampling type: \code{"bootstrap"} or \code{"subsample"}.
#' @param family Model family passed to \code{build_paths}: \code{"gaussian"} or \code{"binomial"}.
#' @param K Maximum number of forward steps for \code{build_paths}.
#' @param eps Minimum AIC improvement for \code{build_paths}.
#' @param delta AIC tolerance for keeping near-tie children in \code{build_paths}.
#' @param L Maximum number of models per level in \code{build_paths}.
#' @param m Subsample size used when \code{resample = "subsample"}.
#'   If \code{NULL}, defaults to \code{ceiling(sqrt(n))}.
#' @param seed Optional integer random seed for reproducibility. If \code{NULL},
#'   the current RNG state is used.
#'
#' @return A list of class \code{"path_stability"} with components:
#'   \item{pi}{Numeric vector of length p with stability scores \eqn{\pi_j} in [0, 1].}
#'   \item{z}{B x p matrix of per-resample proportions \eqn{z_j^{(b)}}.}
#'   \item{meta}{List with resampling and model-selection parameters.}
#'
#' @details
#' For each resample \eqn{b = 1, \dots, B}:
#' \enumerate{
#'   \item Draw a bootstrap sample or subsample of the rows of \code{x, y}.
#'   \item Run \code{build_paths()} on the resampled data.
#'   \item Collect all models from the resulting forest (excluding the empty model at step 0).
#'   \item For each predictor \eqn{j}, compute \eqn{z_j^{(b)}} as the proportion of those models
#'         in which predictor \eqn{j} appears.
#' }
#' Finally, the stability scores are
#' \deqn{\pi_j = \frac{1}{B} \sum_{b=1}^B z_j^{(b)}.}
#'
#' @examples
#' \dontrun{
#'   set.seed(1)
#'   n <- 120; p <- 6
#'   X <- matrix(rnorm(n * p), n, p)
#'   colnames(X) <- paste0("x", 1:p)
#'   y <- X[,1] * 2 - X[,2] + rnorm(n)
#'
#'   stab <- stability(
#'     x = X, y = y,
#'     B = 30,
#'     resample = "bootstrap",
#'     family = "gaussian",
#'     K = min(ncol(X), 5),
#'     eps = 1e-6, delta = 1, L = 20
#'   )
#'
#'   stab$pi  # stability scores per predictor
#' }
#'
#' @export
stability <- function(x, y,
                      B = 50,
                      resample = c("bootstrap", "subsample"),
                      family = c("gaussian", "binomial"),
                      K = min(ncol(x), 10),
                      eps = 1e-6,
                      delta = 1,
                      L = 50,
                      m = NULL,
                      seed = 28) {

  ## ---- Argument checks and setup ----

  resample <- match.arg(resample)
  family   <- match.arg(family)

  # Coerce x to data.frame and ensure column names
  if (is.matrix(x)) {
    x <- as.data.frame(x)
  } else {
    x <- as.data.frame(x)
  }

  if (is.null(colnames(x))) {
    colnames(x) <- paste0("x", seq_len(ncol(x)))
  }

  n <- NROW(x)
  p <- NCOL(x)

  if (length(y) != n) {
    stop("Length of y must match number of rows in x.")
  }

  # Default subsample size if needed
  if (resample == "subsample" && is.null(m)) {
    # Typical choice: m â‰ˆ sqrt(n)
    m <- ceiling(sqrt(n))
  }

  if (!is.null(m) && m > n) {
    stop("Subsample size m cannot exceed n.")
  }

  # Save and optionally set RNG seed
  if (!is.null(seed)) {
    old_seed <- .Random.seed
    on.exit({
      # Restore old RNG state (best-effort)
      if (exists("old_seed", inherits = FALSE)) {
        .Random.seed <<- old_seed
      }
    }, add = TRUE)
    set.seed(seed)
  }

  var_names <- colnames(x)

  # Matrix to store per-resample proportions z_j^(b)
  z_mat <- matrix(0, nrow = B, ncol = p)
  colnames(z_mat) <- var_names

  ## ---- Main resampling loop ----

  for (b in seq_len(B)) {

    # Draw resample indices
    if (resample == "bootstrap") {
      # Sample n rows with replacement
      idx <- sample.int(n, size = n, replace = TRUE)
    } else {  # "subsample"
      # Sample m rows without replacement
      idx <- sample.int(n, size = m, replace = FALSE)
    }

    x_b <- x[idx, , drop = FALSE]
    y_b <- y[idx]

    # Run multi-path forward selection on this resample
    forest_b <- build_paths(
      x      = x_b,
      y      = y_b,
      family = family,
      K      = K,
      eps    = eps,
      delta  = delta,
      L      = L
    )

    # Extract all models from this forest
    mdl_df <- forest_b$aic_by_model

    # We typically exclude the empty model at step 0 from the denominator,
    # because it contains no predictors and does not contribute information
    # about variable selection.
    if ("step" %in% names(mdl_df)) {
      mdl_df <- mdl_df[mdl_df$step > 0, , drop = FALSE]
    }

    # If no models beyond the intercept were produced, z^(b) is all zeros
    if (nrow(mdl_df) == 0L) {
      z_mat[b, ] <- 0
      next
    }

    # mdl_df$vars is expected to be a list column of character vectors
    model_vars_list <- mdl_df$vars
    num_models <- length(model_vars_list)

    # Initialize counts of appearances per predictor
    counts <- integer(p)

    # For each model, update counts for predictors that appear in that model
    for (m_idx in seq_len(num_models)) {
      vars_m <- model_vars_list[[m_idx]]
      # Logical vector: is each predictor in vars_m?
      present <- var_names %in% vars_m
      counts <- counts + as.integer(present)
    }

    # Convert counts to proportions for this resample
    z_mat[b, ] <- counts / num_models
  }

  ## ---- Aggregate across resamples to get pi ----

  # Average over b = 1..B
  pi_vec <- colMeans(z_mat, na.rm = TRUE)

  # Basic sanity check: pi should lie in [0, 1]
  if (any(pi_vec < 0 - 1e-8 | pi_vec > 1 + 1e-8, na.rm = TRUE)) {
    warning("Some stability scores are outside [0, 1]; check z-matrix computation.")
  }

  ## ---- Assemble return object ----

  meta <- list(
    B        = B,
    resample = resample,
    family   = family,
    K        = K,
    eps      = eps,
    delta    = delta,
    L        = L,
    m        = if (resample == "subsample") m else n,
    n        = n,
    p        = p,
    var_names = var_names
  )

  out <- list(
    pi   = pi_vec,
    z    = z_mat,
    meta = meta
  )
  class(out) <- "path_stability"

  out
}

```





# plausible_models


```{r}
#' Plausible model selection using AIC and stability
#'
#' Given a multi-path forest from \code{build_paths()} and a stability vector
#' \eqn{\pi} from \code{stability()}, this function selects a small set of
#' "plausible" models that are both:
#' \enumerate{
#'   \item close to the best model in terms of AIC (within a tolerance \code{Delta}),
#'   \item built from predictors that are, on average, sufficiently stable
#'         (mean stability at least \code{tau}).
#' }
#' Optionally, highly similar models (by Jaccard overlap) can be removed, and
#' fitted model objects can be returned for prediction.
#'
#' @param forest Object returned by \code{build_paths()} (class \code{"path_forest"}).
#' @param pi Numeric vector of stability scores \eqn{\pi_j} for each predictor.
#'   Should either be named by predictor names, or have length equal to
#'   \code{ncol(x)} used in \code{build_paths()} and correspond to the same order.
#' @param Delta Numeric, AIC tolerance for plausibility:
#'   keep models with AIC \eqn{\le AIC_{\min} + \Delta}.
#' @param tau Numeric in [0, 1], threshold on average stability:
#'   keep models whose mean stability \eqn{\pi(S)} is at least \code{tau}.
#' @param jaccard_threshold Optional numeric in (0, 1). If not \code{NULL},
#'   models whose Jaccard similarity with an already-kept model exceeds this
#'   threshold are dropped as near-duplicates.
#' @param x Optional predictor data used to refit models when \code{return_fits = TRUE}.
#'   Only required if \code{return_fits = TRUE}.
#' @param y Optional response vector used to refit models when \code{return_fits = TRUE}.
#'   Only required if \code{return_fits = TRUE}.
#' @param family Model family to use when refitting: \code{"gaussian"} or \code{"binomial"}.
#'   Ignored if \code{return_fits = FALSE}.
#' @param return_fits Logical; if \code{TRUE}, refit each plausible model and
#'   include a list-column \code{fit} containing the fitted objects. Default is \code{FALSE}.
#'
#' @return A data.frame (with class \code{"plausible_models"}) containing one row
#'   per plausible model, with at least the columns:
#'   \itemize{
#'     \item \code{key}: model key (sorted variable names concatenated).
#'     \item \code{step}: forward-selection step at which the model appears.
#'     \item \code{size}: number of predictors in the model.
#'     \item \code{aic}: AIC of the model (from \code{build_paths()}).
#'     \item \code{mean_stability}: average stability score \eqn{\pi(S)}.
#'     \item \code{vars}: list-column of character vectors with predictor names.
#'   }
#'   If \code{return_fits = TRUE}, a list-column \code{fit} is added.
#'
#' @examples
#' \dontrun{
#'   set.seed(1)
#'   n <- 120; p <- 6
#'   X <- matrix(rnorm(n * p), n, p)
#'   colnames(X) <- paste0("x", 1:p)
#'   y <- X[,1] * 2 - X[,2] + rnorm(n)
#'
#'   forest <- build_paths(
#'     x = X, y = y, family = "gaussian",
#'     K = min(ncol(X), 5), eps = 1e-6, delta = 1, L = 25
#'   )
#'   stab <- stability(
#'     x = X, y = y, B = 30, resample = "bootstrap",
#'     family = "gaussian", K = 5, eps = 1e-6, delta = 1, L = 25
#'   )
#'
#'   plaus <- plausible_models(
#'     forest, pi = stab$pi,
#'     Delta = 2, tau = 0.6
#'   )
#'   plaus
#' }
#'
#' @export
plausible_models <- function(forest,
                             pi,
                             Delta = 2,
                             tau = 0.6,
                             jaccard_threshold = NULL) {

  ## ---- Basic checks ----

  if (is.null(forest$aic_by_model)) {
    stop("Argument 'forest' must be an object returned by build_paths() with aic_by_model component.")
  }

  models_df <- forest$aic_by_model

  # We assume models_df has at least columns:
  #   - key   : character key for the model
  #   - aic   : numeric AIC
  #   - vars  : list-column of character vectors of predictors
  #   - step  : step index
  if (!all(c("key", "aic", "vars") %in% names(models_df))) {
    stop("forest$aic_by_model must contain columns 'key', 'aic' and 'vars'.")
  }

  # Drop any duplicated models by key, keeping the lowest AIC version
  # (safety check; build_paths should already avoid duplicates by step).
  if (anyDuplicated(models_df$key)) {
    ord <- order(models_df$key, models_df$aic)
    models_df <- models_df[ord, , drop = FALSE]
    models_df <- models_df[!duplicated(models_df$key), , drop = FALSE]
  }

  # Compute global minimum AIC across all models in the forest
  aic_min <- min(models_df$aic)

  # Filter by AIC tolerance: keep models within Delta of best AIC
  keep_aic <- models_df$aic <= (aic_min + Delta)
  models_df <- models_df[keep_aic, , drop = FALSE]

  if (nrow(models_df) == 0L) {
    warning("No models satisfy the AIC criterion (Delta). Returning empty result.")
    out <- models_df
    class(out) <- c("plausible_models", class(out))
    return(out)
  }

  ## ---- Align stability vector pi with predictor names ----

  # Get predictor names from forest meta if available, otherwise from vars
  if (!is.null(forest$meta$var_names)) {
    var_names <- forest$meta$var_names
  } else {
    # Fallback: use sorted unique variable names across models
    var_names <- sort(unique(unlist(models_df$vars)))
  }

  p <- length(var_names)

  # Ensure pi has correct length and names
  if (!is.null(names(pi)) && all(var_names %in% names(pi))) {
    # Align by names, and reorder to var_names
    pi_named <- pi[var_names]
  } else {
    # Assume pi is in the same order as var_names
    if (length(pi) != p) {
      stop("Length of 'pi' does not match the number of predictors in the forest.")
    }
    pi_named <- pi
    names(pi_named) <- var_names
  }

  ## ---- Compute mean stability for each model ----

  # Initialize vector of mean stabilities
  mean_stab <- numeric(nrow(models_df))

  for (i in seq_len(nrow(models_df))) {
    vars_i <- models_df$vars[[i]]

    if (length(vars_i) == 0L) {
      # The empty model has no predictors; we set its mean stability to NA
      mean_stab[i] <- NA_real_
    } else {
      # Use only stable variables that exist in pi_named
      mean_stab[i] <- mean(pi_named[vars_i], na.rm = TRUE)
    }
  }

  models_df$mean_stability <- mean_stab

  # Drop empty models or those with undefined/NaN stability
  models_df <- models_df[!is.na(models_df$mean_stability), , drop = FALSE]

  # Apply tau threshold: keep models with mean_stability >= tau
  keep_tau <- models_df$mean_stability >= tau
  models_df <- models_df[keep_tau, , drop = FALSE]

  if (nrow(models_df) == 0L) {
    warning("No models satisfy the stability criterion (tau). Returning empty result.")
    out <- models_df
    class(out) <- c("plausible_models", class(out))
    return(out)
  }

  ## ---- Optional: remove near-duplicates using Jaccard similarity ----

  # Helper to compute Jaccard similarity between two sets of variables
  jaccard <- function(a, b) {
    a <- unique(a); b <- unique(b)
    inter <- length(intersect(a, b))
    union <- length(union(a, b))
    if (union == 0L) return(0)
    inter / union
  }

  if (!is.null(jaccard_threshold)) {
    if (jaccard_threshold <= 0 || jaccard_threshold >= 1) {
      stop("jaccard_threshold must be in (0, 1) if provided.")
    }

    # Sort models by AIC (best first); tie-break by larger mean_stability
    ord <- order(models_df$aic, -models_df$mean_stability)
    models_df <- models_df[ord, , drop = FALSE]

    keep_idx <- logical(nrow(models_df))
    kept_vars <- list()

    for (i in seq_len(nrow(models_df))) {
      vars_i <- models_df$vars[[i]]

      # Compare with all previously kept models
      if (length(kept_vars) == 0L) {
        keep_idx[i] <- TRUE
        kept_vars[[1]] <- vars_i
      } else {
        too_similar <- FALSE
        for (kv in kept_vars) {
          if (jaccard(vars_i, kv) > jaccard_threshold) {
            too_similar <- TRUE
            break
          }
        }
        if (!too_similar) {
          keep_idx[i] <- TRUE
          kept_vars[[length(kept_vars) + 1L]] <- vars_i
        }
      }
    }

    models_df <- models_df[keep_idx, , drop = FALSE]
  }

  if (nrow(models_df) == 0L) {
    warning("All models were removed by Jaccard similarity filtering. Returning empty result.")
    out <- models_df
    class(out) <- c("plausible_models", class(out))
    return(out)
  }

  # Reindex models with a simple model_id if desired
  models_df$model_id <- seq_len(nrow(models_df))

  # Move some columns to a nicer order
  col_order <- c(
    "model_id",
    setdiff(
      c("key", "step", "size", "aic", "mean_stability", "vars", "fit"),
      "model_id"
    )
  )
  col_order <- intersect(col_order, names(models_df))
  models_df <- models_df[, c(col_order, setdiff(names(models_df), col_order)), drop = FALSE]

  class(models_df) <- c("plausible_models", class(models_df))
  models_df
}
```






# confusion_metrics


```{r}
#' Confusion matrix and classification metrics for logistic models
#'
#' Compute a confusion matrix and standard classification metrics at a given
#' probability cutoff (default 0.5) for binary outcomes. This is mainly intended
#' for evaluating logistic regression models in the vignette.
#'
#' The function expects observed binary outcomes \code{y} and predicted
#' probabilities \code{prob} for the positive class.
#'
#' @param y Observed binary response. Can be:
#'   \itemize{
#'     \item numeric (0/1),
#'     \item logical (\code{FALSE}/\code{TRUE}),
#'     \item factor with 2 levels (the second level is treated as "positive").
#'   }
#' @param prob Numeric vector of predicted probabilities for the positive class,
#'   of the same length as \code{y}.
#' @param cutoff Numeric cutoff in (0, 1) used to classify predictions as
#'   positive; default is 0.5.
#'
#' @return A list with components:
#'   \item{confusion}{2x2 matrix with rows = observed (0/1), columns = predicted (0/1),
#'         in the order: 0 = negative, 1 = positive.}
#'   \item{metrics}{Named numeric vector with:
#'         \code{prevalence}, \code{accuracy}, \code{sensitivity},
#'         \code{specificity}, \code{FDR}, \code{DOR}.}
#'
#' @details
#' The metrics are defined as:
#' \itemize{
#'   \item \code{prevalence} = (TP + FN) / N
#'   \item \code{accuracy}   = (TP + TN) / N
#'   \item \code{sensitivity} (recall, TPR) = TP / (TP + FN)
#'   \item \code{specificity} (TNR) = TN / (TN + FP)
#'   \item \code{FDR} (false discovery rate) = FP / (TP + FP)
#'   \item \code{DOR} (diagnostic odds ratio) =
#'         (TP / FN) / (FP / TN) = (TP * TN) / (FP * FN)
#' }
#' When a denominator is zero (e.g. no positives, no predicted positives, or a
#' cell count is zero), the corresponding metric is returned as \code{NA}.
#'
#' @examples
#' \dontrun{
#'   set.seed(1)
#'   n <- 200
#'   x1 <- rnorm(n)
#'   x2 <- rnorm(n)
#'   linpred <- 1 + 1.5 * x1 - 2 * x2
#'   p <- 1 / (1 + exp(-linpred))
#'   y <- rbinom(n, 1, p)
#'
#'   fit <- glm(y ~ x1 + x2, family = binomial())
#'   prob <- predict(fit, type = "response")
#'
#'   cm <- confusion_metrics(y, prob, cutoff = 0.5)
#'   cm$confusion
#'   cm$metrics
#' }
#'
#' @export
confusion_metrics <- function(y, prob, cutoff = 0.5) {

  ## ---- Input checks ----

  if (length(y) != length(prob)) {
    stop("y and prob must have the same length.")
  }

  if (!is.numeric(prob)) {
    stop("prob must be a numeric vector of predicted probabilities.")
  }

  if (any(is.na(y)) || any(is.na(prob))) {
    stop("Missing values in y or prob are not allowed.")
  }

  if (cutoff <= 0 || cutoff >= 1) {
    stop("cutoff must be strictly between 0 and 1.")
  }

  # Convert y to numeric 0/1
  y_bin <- NULL

  if (is.factor(y)) {
    if (nlevels(y) != 2L) {
      stop("If y is a factor, it must have exactly 2 levels.")
    }
    # Treat the second level as positive (1)
    y_bin <- as.integer(y == levels(y)[2L])
  } else if (is.logical(y)) {
    y_bin <- as.integer(y)
  } else if (is.numeric(y)) {
    # We tolerate numeric 0/1; otherwise stop
    uniq <- sort(unique(y))
    if (!all(uniq %in% c(0, 1))) {
      stop("Numeric y must only contain 0 and 1.")
    }
    y_bin <- as.integer(y)
  } else {
    stop("y must be numeric (0/1), logical, or a 2-level factor.")
  }

  ## ---- Construct predicted classes ----

  # Predicted positive if prob >= cutoff
  y_hat <- as.integer(prob >= cutoff)

  ## ---- Compute confusion matrix components ----

  # Confusion counts:
  # TN = true negatives (y = 0, y_hat = 0)
  # FP = false positives (y = 0, y_hat = 1)
  # FN = false negatives (y = 1, y_hat = 0)
  # TP = true positives (y = 1, y_hat = 1)
  TN <- sum(y_bin == 0 & y_hat == 0)
  FP <- sum(y_bin == 0 & y_hat == 1)
  FN <- sum(y_bin == 1 & y_hat == 0)
  TP <- sum(y_bin == 1 & y_hat == 1)

  N <- TN + FP + FN + TP

  # Build a 2x2 confusion matrix with rows = observed, cols = predicted
  confusion <- matrix(
    c(TN, FP, FN, TP),
    nrow = 2, ncol = 2, byrow = TRUE,
    dimnames = list(
      observed  = c("0", "1"),
      predicted = c("0", "1")
    )
  )

  ## ---- Compute metrics ----

  # Prevalence: proportion of positives in the sample
  prevalence <- if (N > 0) (TP + FN) / N else NA_real_

  # Accuracy: proportion of correct predictions
  accuracy <- if (N > 0) (TP + TN) / N else NA_real_

  # Sensitivity (recall, TPR): TP / (TP + FN)
  denom_sens <- TP + FN
  sensitivity <- if (denom_sens > 0) TP / denom_sens else NA_real_

  # Specificity (TNR): TN / (TN + FP)
  denom_spec <- TN + FP
  specificity <- if (denom_spec > 0) TN / denom_spec else NA_real_

  # False discovery rate (FDR): FP / (TP + FP)
  denom_fdr <- TP + FP
  FDR <- if (denom_fdr > 0) FP / denom_fdr else NA_real_

  # Diagnostic odds ratio (DOR): (TP/FN) / (FP/TN) = (TP * TN) / (FP * FN)
  # Only defined when FP > 0 and FN > 0 and TN > 0 and TP > 0.
  if (TN > 0 && TP > 0 && FP > 0 && FN > 0) {
    DOR <- (TP * TN) / (FP * FN)
  } else {
    DOR <- NA_real_
  }

  metrics <- c(
    prevalence  = prevalence,
    accuracy    = accuracy,
    sensitivity = sensitivity,
    specificity = specificity,
    FDR         = FDR,
    DOR         = DOR
  )

  ## ---- Return ----

  list(
    confusion = confusion,
    metrics   = metrics
  )
}

```









## test

## linear

```{r}
set.seed(1)
n <- 120; p <- 8
X <- matrix(rnorm(n*p), n, p)
beta <- c(2, -1.5, 0, 0, 1, rep(0, p-5))
y <- X %*% beta + rnorm(n, sd = 1)
colnames(X) <- paste0("x", 1:p)
df <- as.data.frame(cbind(y, X))
```


```{r}
forest <- build_paths(x = X, y = as.numeric(y),
                      family = "gaussian",
                      K = min(ncol(X), 10),
                      eps = 1e-6, delta = 1, L = 50)

stab <- stability(x = X, y = as.numeric(y),
                  B = 50, resample = "bootstrap",
                  family = "gaussian",
                  K = 10, eps = 1e-6, delta = 1, L = 50)

plaus <- plausible_models(forest,
                          pi    = stab$pi,
                          Delta = 2,
                          tau   = 0.6)

plaus
```

## logistic

```{r}
set.seed(2)
n <- 200; p <- 6
Xb <- matrix(rnorm(n*p), n, p)
linpred <- 1.2*Xb[,1] - 1*Xb[,2] + 0.8*Xb[,5]
prob <- 1 / (1 + exp(-linpred))
ybin <- rbinom(n, 1, prob)
colnames(Xb) <- paste0("x", 1:p)
dfb <- as.data.frame(cbind(y = ybin, Xb))
```


```{r}
forest_b <- build_paths(
  x = Xb,
  y = ybin,
  family = "binomial",
  K = min(ncol(Xb), 10),
  eps = 1e-6,
  delta = 1,
  L = 50
)

stab_b <- stability(
  x = Xb,
  y = ybin,
  B = 50,
  resample = "bootstrap",
  family = "binomial",
  K = 10,
  eps = 1e-6,
  delta = 1,
  L = 50
)

plaus_b <- plausible_models(
  forest = forest_b,
  pi     = stab_b$pi,
  Delta  = 2,     # AIC tolerance
  tau    = 0.6    # min avg stability
)


plaus_b
```


```{r}
set.seed(2)
n <- 200; p <- 6

# simulate predictors
Xb <- matrix(rnorm(n * p), n, p)

# linear predictor used to generate true probabilities
linpred <- 1.2 * Xb[, 1] - 1 * Xb[, 2] + 0.8 * Xb[, 5]

# true probabilities
prob <- 1 / (1 + exp(-linpred))

# simulate binary outcome
ybin <- rbinom(n, 1, prob)

# put into a data.frame
colnames(Xb) <- paste0("x", 1:p)
dfb <- as.data.frame(cbind(y = ybin, Xb))

# fit logistic regression model
fit <- glm(y ~ ., family = binomial(), data = dfb)

# predicted probabilities from the model
phat <- predict(fit, type = "response")

# use your confusion_metrics() helper at cutoff 0.5
cm <- confusion_metrics(dfb$y, phat, cutoff = 0.5)

cm$confusion
cm$metrics
```


























